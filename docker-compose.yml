version: '2' 
services:
  namenode:
    image: bde2020/hadoop-namenode:1.1.0-hadoop2.7.1
    hostname: namenode
    container_name: namenode
    domainname: hadoop
    depends_on:
      - nodemanager1
    networks:
      - hadoop
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - "50070:50070"
      - "8020:8020"

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:1.1.0-hadoop2.7.1
    hostname: resourcemanager
    container_name: resourcemanager
    domainname: hadoop
    depends_on:
      - namenode
      - datanode1
      - datanode2
    ports:
      - "9200:8088"
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
    networks: 
      - hadoop
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:1.1.0-hadoop2.7.1
    hostname: historyserver
    container_name: historyserver
    domainname: hadoop
    networks: 
      - hadoop
    volumes:
      - ./data/historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:1.1.0-hadoop2.7.1
    hostname: nodemanager1    
    container_name: nodemanager1
    domainname: hadoop
    networks: 
      - hadoop
    env_file:
      - ./hadoop.env

  datanode1:
    image: bde2020/hadoop-datanode:1.1.0-hadoop2.7.1
    hostname: datanode1
    container_name: datanode1
    domainname: hadoop
    depends_on:
      - namenode
    networks:
      - hadoop
    volumes:
      - ./data/datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  datanode2:
    image: bde2020/hadoop-datanode:1.1.0-hadoop2.7.1
    hostname: datanode2
    container_name: datanode2
    domainname: hadoop
    depends_on:
      - namenode
    networks: 
      - hadoop
    volumes:
      - ./data/datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  spark-master:
    image: miciav/hadoop-spark-master
    hostname: spark-master
    container_name: spark-master
    domainname: hadoop
    networks:
      - hadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - SPARK_CONF_spark_eventLog_enabled=true
      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:8020/spark-logs
      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs
    env_file:
      - ./hadoop.env
    ports:
        - "8080:8080"
        - "4040:4040"

  spark-worker:
    image: miciav/hadoop-spark-worker
    hostname: spark-worker
    domainname: hadoop
    networks: 
      - hadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - SPARK_CONF_spark_eventLog_enabled=true
      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:8020/spark-logs
      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs
      - SPARK_MASTER_URL=spark://spark-master:7077
    env_file:
      - ./hadoop.env

  zeppelin:
    container_name: zeppelin
    image: miciav/hadoop-spark-zeppelin
    environment:
      ZEPPELIN_PORT: 8090
      MASTER: spark://spark-master:7077
        - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
        - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
        - SPARK_CONF_spark_eventLog_enabled=true
        - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:8020/spark-logs
        - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs
        - ZEPPELIN_JAVA_OPTS="-Dhdp.version=2.7.1"
#      KAFKA_ADDRESS: kafka:9092
#    depends_on:
#      - kafka
#      - zookeeper
#      - cassandra
#    links:
#      - cassandra
#      - master
    depends_on:
      - namenode
      - resourcemanager
    ports:
      - "9100:8090"
      - "8180:8080"
    volumes:
      - ./zeppelin/about.json:/usr/zeppelin/notebook/about.json
      - ./zeppelin/notebooks:/usr/zeppelin/notebook
      - ./zeppelin/conf:/usr/zeppelin/conf
    networks:      
      - hadoop
    env_file:
      - ./hadoop.env


#  spark-notebook:
#    image: earthquakesan/hadoop-spark-notebook:1.0.0
#    hostname: sparknotebook
#    container_name: sparknotebook
#    domainname: hadoop
#    networks:
#      - hadoop
#    environment:
#      - SPARK_NOTEBOOK_MASTER=yarn-client
#    env_file:
#      - ./hadoop.env
#    ports:
#      - "9000:9000"

  # hue:
  #   image: bde2020/hdfs-filebrowser:3.9
  #   hostname: hdfsfb
  #   container_name: hdfsfb
  #   domainname: hadoop
  #   networks:
  #     - hadoop
  #   environment:
  #     - NAMENODE_HOST=namenode
  #   ports:
  #     - "8088:8088"

networks:
  hadoop:
    external: true
